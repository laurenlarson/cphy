{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4f94c8",
   "metadata": {},
   "source": [
    "Preamble and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee03d2d-906b-4dee-aee1-b48f2a17a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80695183",
   "metadata": {},
   "source": [
    "# Ridge regression and learning operators\n",
    "\n",
    "You've likely encountered linear regression before in the context of fitting lines to data; it represents an interpretable \"null hypothesis\" model for finding trends in data. However, here we want to think a little more about how to interpret and understand linear regression, and so we will study a different context: learning approximate dynamical models directly from observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693713e8-32a9-4cdc-b26d-2452ac17bd32",
   "metadata": {},
   "source": [
    "### The vortex street revisited\n",
    "\n",
    "We've already seen the von Karman vortex street in a previous homework assignment. This fluid flow corresponds a uniform flow moving over a cylinder. The velocity scale of this system, $U$, is set by the speed at which the fluid passes over the cylinder, while the length scale $L$ is the length of the cylinder. We normally see the vortex street instability first appear in the wake when $Re \\gtrsim 100$. In our previous homework, we use a von Karman dataset comprising a series of snapshots of a two-dimensional velocity field, with separate datasets corresponding to different Reynolds numbers. \n",
    "\n",
    "We want to train a model that predicts the next snapshot of the flow field, $\\mathbf{v}_{t+1}$, given only the current snapshot $\\mathbf{v}_t$ as an input.\n",
    "Our general learning problem has the form \n",
    "$$\n",
    "\\mathbf{v}(\\mathbf{r})_{t + 1} = \\mathbf{f}(\\mathbf{v}(\\mathbf{r})_{t})\n",
    "$$\n",
    "where $\\mathbf{f}$ is the function that we are attempting to learn. While there are a variety of methods for learning the function $\\mathbf{f}$, we will start by attempting to learn a linear model,\n",
    "$$\n",
    "\\mathbf{v}(\\mathbf{r})_{t + 1} = \\boldsymbol{\\theta} \\cdot \\mathbf{v}_{t}\n",
    "$$\n",
    "This problem is challenging because the Navier-Stokes equations are not linear in $\\mathbf{v}$. In other words, by fitting a linear model to the snapshot time series, we attempt to approximate a highly nonlinear PDE with a linear model. We suspect that this approximation may work better over shorter timescales.\n",
    "\n",
    "We can treat this problem as a regression problem, where our data matrix $X$ represents a stack of snapshots of the velocity field at different times, $X^T = [\\mathbf{v}_1\\;\\mathbf{v}_2\\;\\cdots\\mathbf{v}_{N-1}]$. While we are working with time series data, we will treat each snapshot as if it were an independent sample of a set of features---our regression results will be invariant to random shuffles of the input velocity field time series. In a more sophisticated approach, we might consider taking the ordering information into account in our regression. Our regression target, $Y$, represents the values of the velocity field at the next point in time, because our model will attempt to just predict the next step ahead in the process. For this reason, our labels or targets represent the matrix $Y^T = [\\mathbf{v}_2\\; \\mathbf{v}_3\\;\\cdots\\mathbf{v}_N]$. We emphasize that this regression problem is identical to standard linear regression, where we predict an outcome $y$ based on a set of input features $X$. Our forecasting problem is unique only in that $X$ and $Y$ happen to have the same dimensionality, since they are snapshots of a velocity field.\n",
    "\n",
    "There's one hiccup in our approach: our input data is extremely high-dimensional. If we think of our input $X$ as a design matrix $X \\in \\mathbb{R}^{N_{data} \\times N_{features}}$, then number of features is extremely large $N_{features} = 2 \\cdot N_x \\cdot N_y$, because we have x- and y- velocity components on each site of an $N_x \\times N_y$ lattice. That means that we might find ourselves in the limit $N_{data} < N_{features}$, in which case the problem is underdetermined: there are multiple values of $\\boldsymbol \\theta$ that will maximize the accuracy of our model. As a tiebreaker, we will use the Ridge, or Tikhonov criterion: we select the solution parameter vector $\\boldsymbol \\theta$ with the lowest norm.\n",
    "\n",
    "Putting our goals together, we arrive at a loss function describing the problem that we want to solve,\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\theta}) = (X \\boldsymbol{\\theta} - \\mathbf{y})^T(X \\boldsymbol{\\theta} - \\mathbf{y}) + \\lambda \\boldsymbol{\\theta}^T \\boldsymbol{\\theta}\n",
    "$$\n",
    "where the first term is proportional to the mean squared error, and $\\lambda$ is a hyperparameter that determines how much we penalize the total weight vector. We wish to minimize this objective function with respect to the parameter matrix $\\boldsymbol{\\theta}$. This means that we want to solve the linear equation $\\frac{\\partial\\mathcal{L}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = 0$ for $\\boldsymbol{\\theta}$.  Note that while the loss function is a scalar, we are taking its derivative with respect to a tensor $\\boldsymbol{\\theta}$. After some linear algebra, we arrive at the following closed-form expression for the value of the weight vector\n",
    "$$\n",
    "\\boldsymbol{\\theta} = (X^T X + \\lambda \\mathbb{I})^{-1} X^T \\;Y\n",
    "$$\n",
    "In the case $\\lambda = 0$, our problem reduces to the ordinary least-squares (OLS) formula. In the case where we have large numbers of features, the ridge penalty is necessary to ensure that our problem has a unique solution. Even if $N_{samples} > N_{features}$, we might still prefer to use ridge regression to ordinary least squares, because ridge regression favors more parsimonious solutions, that avoid excessively weighting certain features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## To Do\n",
    "\n",
    "+ Derive the ridge regression formula shown above. You may find some of the linear algebra tricks we saw in the [matrix cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) helpful here.\n",
    "+ Implement a Ridge Regression model and predict the next values of the Navier-Stokes equations. You should download the [von Karman dataset](https://utexas.box.com/s/44f89zfy7v2k4g5wq4kv3vfvkm9w91wm) and place it in the top-level directory `cphy/resources/`. I've included an object below that will handle creating train/val/test splits for you, and I've also included placeholder code to get you started on the regression model. For now, just implement the \"global\" solution method to the ridge regression problem. You can implement an iterative solution if you want to try something harder.\n",
    "+ You'll notice that my train/test split in the ForecastingDataset object ensures that the test data occurs after the training data in the time series, and that no data points are repeated. Why do you think this is important for this problem?\n",
    "+ Our model has a single hyperparameter, the regularizer strength $\\lambda$, that determines the degree to which the weight entries in the regression matrix are penalized. Using the validation fold of our training dataset, find the best value of that hyperparameter.\n",
    "+ Repeat your experiments using the von Karman datasets with different Reynolds numbers. How does forecast accuracy generally change with Reynolds number?\n",
    "+ (Hard, optional) We are treating our forecasting problem like a black box regression problem---we seek to find a linear map between two vectors, which happen to be flattened snapshots of the velocity field. However, since we know that our problem is a fluid flow, we can exploit our domain knowledge to improve how our model sees the data---in the language of ML, we can improve our model with an \"inductive bias.\" From the numerical integration homework, we know that solving spatial PDEs involves calculating discrete Laplace operators, or taking the spatial Fourier transform. Using my code outlined below, implement a function that featurizes each snapshot of a velocity field array by adding spatial gradients and frequencies as features. Re-run your forecasting with this improved featurization of the data.\n",
    "+ There are plenty of other ways to solve a regression problem. In the last code cells below, we use implementations of several regression models included in the Python package `scikit-learn``. Check out how these models perform. Are there any surprises? Look up some background info on the models that seem to perform particularly well. Why do you think they work so well on this problem?\n",
    "\n",
    "## Optional advanced problems\n",
    "\n",
    "+ You'll notice that we downsampled our velocity field data. That's because the matrix operations needed to calculate the weight matrix become burdensome when the dataset is sufficiently large. Instead of directly solving our objective function with matrix algebra, we can instead iteratively solve it using gradient descent. Try implementing a gradient descent solution to the linear regression problem. For my version of the solution, I added a private method to the LinearRegressor object, and a keyword argument to the constructor that chooses between global and iterative solving. \n",
    "+ If you want to even further improve your iterative method, try adding momentum and adaptive step size to your gradient descent. Here are [some examples of common optimizers](https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html) used to train machine learning models like neural networks\n",
    "+ Advanced: Instead of the ridge penalty, we can use the Lasso, or L1, regularization term: $$\\mathcal{L}(\\theta) = \\sum_{i} |\\theta_i|$$. This is a little bit trickier to implement than ridge regression, because the loss is not differentiable at $0$.\n",
    "+ A mathematical problem: We essentially fit a linear operator $\\mathbf{v}_{t + 1} = A \\mathbf{v}_{t}$. Starting from the non-dimensional Navier-Stokes equations, derive an expression for the leading order linear behavior of the equations, as well as the first-order nonlinear term $\\mathcal{O}(\\mathbf{v}^T \\mathbf{v})$. How does accuracy of the linear approximation depend on the Reynolds number?\n",
    "\n",
    "## Additional information\n",
    "\n",
    "\n",
    "+ Applications of deep learning to flow field modelling and prediction is an active area of research. Recent works propose [physics-informed neural networks](https://www.science.org/doi/abs/10.1126/science.aaw4741), where the same differentiability that faciliates gradient descent also allows calculation of gradient terms in the governing equations for the fluid flow.\n",
    "+ Our ridge regression penalty represents a form of *regularization*, which helps prevent our model from overfitting. The ridge regularizer is commonly used even for more sophisticated models like deep neural networks, in order to constrain solutions to have simpler forms (even at the expense of overall accuracy). Many other regularizers exist, which serve to encourage certain desiderata in model space, such as representational uniqueness or sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b687a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Load the velocity field data\n",
    "\n",
    "Re = 600 # Reynolds number, change this to 300, 600, 900, 1200\n",
    "\n",
    "# Load the two-dimensional velocity field data. Data is stored in a 4D numpy array,\n",
    "# where the first dimension is the time index, the second and third dimensions are the\n",
    "# x and y coordinates, and the fourth dimension is the velocity components (ux or uv).\n",
    "vfield = np.load(\n",
    "    f\"../resources/von_karman_street/vortex_street_velocities_Re_{Re}_largefile.npz\", \n",
    "    allow_pickle=True\n",
    ")\n",
    "# downsample the data for faster training\n",
    "vfield = vfield[::8, ::4, ::4]\n",
    "print(\"Velocity field data has shape: {}\".format(vfield.shape))\n",
    "\n",
    "# Compute the velocity magnitude\n",
    "vfield_mag = np.sqrt(vfield[..., 0]**2 + vfield[..., 1]**2)\n",
    "\n",
    "n_tpts = vfield.shape[0]\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(8):\n",
    "    plt.subplot(1, 8, i+1)\n",
    "    v_vals = vfield_mag[n_tpts // 8 * i]\n",
    "    plt.imshow(v_vals, cmap=\"viridis\", vmin=0, vmax=np.percentile(v_vals, 99))\n",
    "    plt.title(\"Timepoint {}\".format(i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastingDataset:\n",
    "    \"\"\"\n",
    "    A class for formatting time series data for forecasting.\n",
    "    By convention, time is assumed to be the first dimension of the dataset.\n",
    "\n",
    "    For time series data, it is very important that all data in test set is after \n",
    "    all data in the train and val sets. We also need to ensure that datapoints don't\n",
    "    appear in multiple splits. \n",
    "\n",
    "    Parameters\n",
    "        X (np.ndarray): The time series data. The first dimension is assumed to be time.\n",
    "        split_ratio (tuple): The ratio of the data to be used for train, val, and test.\n",
    "        forecast_horizon (int): The number of time steps to forecast at once\n",
    "        featurizer (callable): A function that takes in multivariate snapshot and \n",
    "            returns a feature vector. If None, the raw data is used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, split_ratio=(0.6, 0.2, 0.2), forecast_horizon=1, featurizer=None):\n",
    "\n",
    "        if featurizer is None:\n",
    "            self.featurizer = lambda x: x\n",
    "        else:\n",
    "            self.featurizer = featurizer\n",
    "\n",
    "        self.feature_shape = X.shape[1:]\n",
    "\n",
    "        # We need to ensure that datapoints don't appear in multiple splits, hence why\n",
    "        # we crop by the forecast horizon. We are going to do one-step forecasting\n",
    "        self.X_full = self.featurizer(X[:-1])#[:-forecast_horizon]\n",
    "        self.y_full = X[1:]#[forecast_horizon:]\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        # Split the data into train, val, test\n",
    "        n_train = int(len(self.X_full) * split_ratio[0])\n",
    "        n_val = int(len(self.X_full) * split_ratio[1])\n",
    "        n_test = len(self.X_full) - n_train - n_val\n",
    "\n",
    "        # Our frequent use of the forecast_horizon parameter again arises from our need\n",
    "        # to ensure that datapoints don't appear in multiple splits.\n",
    "        self.X_train, self.y_train = self.X_full[:n_train], self.y_full[:n_train]\n",
    "        self.X_val, self.y_val = (\n",
    "            self.X_full[n_train + forecast_horizon : n_train + forecast_horizon + n_val], # 1 - 600, 2 - 601 | 601 - 800\n",
    "            self.y_full[n_train+ forecast_horizon:n_train + forecast_horizon + n_val]\n",
    "        )\n",
    "        self.X_test, self.y_test = (\n",
    "            self.X_full[n_train + 2 * forecast_horizon+ n_val:], \n",
    "            self.y_full[n_train + 2 * forecast_horizon + n_val:]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_full)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_full[idx], self.y_full[idx]\n",
    "\n",
    "    def flatten_data(self, x):\n",
    "        \"\"\"\n",
    "        Given a dataset, transform into a flat feature form\n",
    "        \"\"\"\n",
    "        return np.reshape(x, (x.shape[0], -1))\n",
    "\n",
    "    def unflatten_data(self, x):\n",
    "        \"\"\"\n",
    "        Given a flat dataset, convert back to the original shape\n",
    "        \"\"\"\n",
    "        out = np.reshape(x, (x.shape[0], *self.feature_shape))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Let's do a simple unit test to make sure that our class is working as expected\n",
    "# Take some time to understand what these test cases cover\n",
    "import unittest\n",
    "class TestForecastingDataset(unittest.TestCase):\n",
    "\n",
    "    def test_initialization(self):\n",
    "        fd = ForecastingDataset(np.arange(100)[:, None])\n",
    "        assert fd.y_train[0] == fd.X_train[1], \"y_train is not shifted by 1 from X_train\"\n",
    "        assert fd.y_val[0] == fd.X_val[1], \"y_val is not shifted by 1 from X_val\"\n",
    "        assert fd.y_test[0] == fd.X_test[1], \"y_test is not shifted by 1 from X_test\"\n",
    "        \n",
    "        assert fd.y_train[-1] < fd.y_val[0], \"y_train and y_test are not disjoint\"\n",
    "        assert fd.y_val[-1] < fd.y_test[0], \"y_val and y_test are not disjoint\"\n",
    "\n",
    "    # test split_size\n",
    "\n",
    "unittest.main(argv=[''], exit=False);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb622993",
   "metadata": {},
   "source": [
    "# Implement the Ridge regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a440c-feb4-4003-99ca-5fece3c4c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class BaseRegressor:\n",
    "    \"\"\"\n",
    "    A base class for regression models.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the model to the data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        #raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "        return X @ self.weights + self.bias\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns the mean squared error of the model.\n",
    "        \"\"\"\n",
    "        return np.mean((self.predict(X) - y)**2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LinearRegressor(BaseRegressor):\n",
    "    \"\"\"\n",
    "    A linear regression model is a linear function of the form:\n",
    "    y = w0 + w1 * x1 + w2 * x2 + ... + wn * xn\n",
    "\n",
    "    The weights are the coefficients of the linear function.\n",
    "    The bias is the constant term w0 of the linear function.\n",
    "\n",
    "    Attributes:\n",
    "        method: str, optional. The method to use for fitting the model.\n",
    "        regularization: str, optional. The type of regularization to use.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method=\"global\", regularization=\"ridge\", regstrength=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.method = method\n",
    "        self.regularization = regularization\n",
    "        self.regstrength = regstrength\n",
    "\n",
    "    # functions that begin with underscores are private, by convention.\n",
    "    # Technically we could access them from outside the class, but we should\n",
    "    # not do that because they can be changed or removed at any time.\n",
    "    def _fit_global(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the model using the global least squares method.\n",
    "        \"\"\"\n",
    "        if self.regularization is None:\n",
    "            self.weights = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "        elif self.regularization == \"ridge\":\n",
    "            self.weights = np.linalg.inv(X.T @ X + np.eye(X.shape[1]) * self.regstrength) @ X.T @ y\n",
    "        else:\n",
    "            warnings.warn(\"Unknown regularization method, defaulting to None\")\n",
    "            self.weights = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "        self.bias = np.mean(y - X @ self.weights)\n",
    "        return self.weights, self.bias\n",
    "\n",
    "    def _fit_iterative(self, X, y, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent.\n",
    "        \"\"\"\n",
    "        self.weights = np.zeros((X.shape[1], X.shape[1]))\n",
    "        self.bias = np.mean(y)\n",
    "        for i in range(X.shape[0]):\n",
    "            self.weights += learning_rate * (y[i] - X[i] @ self.weights - self.bias) * X[i] - self.regstrength * self.weights\n",
    "        self.weights /= X.shape[0]\n",
    "        return self.weights, self.bias\n",
    "        #raise NotImplementedError\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the model to the data. The method used is determined by the\n",
    "        `method` attribute.\n",
    "        \"\"\"\n",
    "        if self.method == \"global\":\n",
    "            out = self._fit_global(X, y)\n",
    "        elif self.method == \"iterative\":\n",
    "            out = self._fit_iterative(X, y)\n",
    "        else:\n",
    "            out = self._fit_global(X, y)\n",
    "        return out\n",
    "        #raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d32ad",
   "metadata": {},
   "source": [
    "## Test your solution\n",
    "\n",
    "+ You don't need to write any new code below here. This is all to test your solution and study its properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import William's solution\n",
    "# from solutions.linear_regression import LinearRegressor\n",
    "\n",
    "# Automatically split the data into train, validation, and test sets\n",
    "dataset= ForecastingDataset(vfield_mag)\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegressor(method=\"global\", regularization=\"ridge\", regstrength=1.0)\n",
    "\n",
    "model.fit(\n",
    "    dataset.flatten_data(dataset.X_train), \n",
    "    dataset.flatten_data(dataset.y_train)\n",
    ")\n",
    "\n",
    "y_test_pred = dataset.unflatten_data(\n",
    "    model.predict(dataset.flatten_data(dataset.X_test))\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(np.hstack(y_test_pred[::3][-10:]))\n",
    "plt.title(\"Predicted snapshots\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(np.hstack(dataset.y_test[::3][-10:]))\n",
    "plt.title(\"True snapshots\")\n",
    "\n",
    "\n",
    "mse_vs_time = np.mean((y_test_pred - dataset.y_test)**2, axis=(1, 2))\n",
    "plt.figure()\n",
    "plt.plot(mse_vs_time)\n",
    "plt.ylabel(\"One step ahead forecast error\")\n",
    "plt.xlabel(\"Time step\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e323f",
   "metadata": {},
   "source": [
    "# Run hyperparameter tuning\n",
    "\n",
    "Using the validation fold of the von Karman dataset, find the best value of the ridge penalty hyperparameter $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regstrengths = np.logspace(-4, 4, 9)\n",
    "\n",
    "all_mse = []\n",
    "for regstrength in regstrengths:\n",
    "    print(f\"Training model with regstrength={regstrength}\", flush=True)\n",
    "\n",
    "    model = LinearRegressor(method=\"global\", regularization=\"ridge\", regstrength=regstrength)\n",
    "\n",
    "    model.fit(\n",
    "        dataset.flatten_data(dataset.X_train), \n",
    "        dataset.flatten_data(dataset.y_train)\n",
    "    )\n",
    "\n",
    "    y_val_pred = dataset.unflatten_data(\n",
    "        model.predict(dataset.flatten_data(dataset.X_val))\n",
    "    )\n",
    "\n",
    "    mse = np.mean((y_val_pred - dataset.y_val)**2)\n",
    "    all_mse.append(mse)\n",
    "\n",
    "best_regstrength = regstrengths[np.argmin(np.array(all_mse))]\n",
    "\n",
    "print(f\"Best regularize strength: {best_regstrength}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995a2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressor(method=\"global\", regularization=\"ridge\", regstrength=best_regstrength)\n",
    "\n",
    "model.fit(\n",
    "    dataset.flatten_data(dataset.X_train), \n",
    "    dataset.flatten_data(dataset.y_train)\n",
    ")\n",
    "\n",
    "y_test_pred = dataset.unflatten_data(\n",
    "    model.predict(dataset.flatten_data(dataset.X_test))\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(np.hstack(y_test_pred[::3][-10:]))\n",
    "plt.title(\"Predicted snapshots\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(np.hstack(dataset.y_test[::3][-10:]))\n",
    "plt.title(\"True snapshots\")\n",
    "\n",
    "mse_vs_time = np.mean((y_test_pred - dataset.y_test)**2, axis=(1, 2))\n",
    "plt.figure()\n",
    "plt.plot(mse_vs_time)\n",
    "plt.ylabel(\"One step ahead forecast error\")\n",
    "plt.xlabel(\"Time step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de57416",
   "metadata": {},
   "source": [
    "# Improve the featurization of the velocity field dataset\n",
    "\n",
    "+ Use domain knowledge to calculate nonlinear features of the velocity field, and see if you can get better results for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab3e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_flowfield(field):\n",
    "    \"\"\"\n",
    "    Compute features of a 2D spatial field. These features are chosen based on the \n",
    "    intuition that the input field is a 2D spatial field with time translation \n",
    "    invariance.\n",
    "\n",
    "    The output is an augmented feature along the last axis of the input field.\n",
    "\n",
    "    Args:\n",
    "        field (np.ndarray): A 3D array of shape (batch, nx, ny) containing the flow \n",
    "            field. The \"batch\" axis corresponds to time or an arbitrary flow field \n",
    "            snapshot index, if you shuffled the data.\n",
    "\n",
    "    Returns:\n",
    "        field_features (np.ndarray): An array of shape (batch, nx, ny, M) containing \n",
    "            the additional computed features stacked along the last axis\n",
    "    \"\"\"\n",
    "    field_fft = np.fft.fft2(field)\n",
    "    field_fft = np.fft.fftshift(field_fft)\n",
    "    field_fft_abs = np.log(np.abs(field_fft) + 1e-8)[..., None]\n",
    "    field_fft_phase = np.angle(field_fft)[..., None]\n",
    "\n",
    "    ## Compute the spatial gradients along x and y\n",
    "    field_grad = np.gradient(field, axis=(-2, -1))\n",
    "    field_grad = np.stack(field_grad, axis=-1)\n",
    "\n",
    "    ## Compute the spatial Laplacian\n",
    "    field_lap = np.stack(np.gradient(field_grad, axis=(-2, -1)), axis=-1)\n",
    "    field_lap = np.sum(field_lap, axis=-1)\n",
    "\n",
    "    field = field[..., None]\n",
    "    field_features = np.concatenate(\n",
    "        [field, field_grad, field_lap, field_fft_phase, field_fft_abs], \n",
    "        axis=-1\n",
    "    )\n",
    "    return field_features\n",
    "    #raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab631c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import William's solutions\n",
    "# from solutions.linear_regression import featurize_flowfield\n",
    "\n",
    "## Pass the data through the featurizer using the keyword argument\n",
    "dataset = ForecastingDataset(vfield_mag, featurizer=featurize_flowfield)\n",
    "\n",
    "## Initialize the model\n",
    "model = LinearRegressor(method=\"global\", regularization=\"ridge\", regstrength=1e5)\n",
    "\n",
    "model.fit(\n",
    "    dataset.flatten_data(dataset.X_train), \n",
    "    dataset.flatten_data(dataset.y_train)\n",
    ")\n",
    "\n",
    "y_test_pred = dataset.unflatten_data(\n",
    "    model.predict(\n",
    "        dataset.flatten_data(dataset.X_test)\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(\n",
    "    np.hstack(y_test_pred[::3][-10:])\n",
    ")\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(\n",
    "    np.hstack(dataset.y_test[::3][-10:])\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.mean((y_test_pred - dataset.y_test)**2, axis=(1, 2)))\n",
    "plt.ylabel(\"One step forecast error\")\n",
    "plt.xlabel(\"Time step\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff3b23",
   "metadata": {},
   "source": [
    "# Try some other regression models\n",
    "\n",
    "Make sure you've installed scikit-learn in your local environment\n",
    "\n",
    "    conda install scikit-learn\n",
    "\n",
    "These models each have their own sets of hyperparameters. We will use the default values, but you will likely get better results by tuning them. Check out the [scikit-learn documentation](https://scikit-learn.org/stable/supervised_learning.html) for more information about the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0705f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "for model in [DecisionTreeRegressor(), KNeighborsRegressor(), \n",
    "                GaussianProcessRegressor(), MLPRegressor(), \n",
    "                LinearRegression(), Ridge(), Lasso(), ElasticNet()\n",
    "            ]:\n",
    "\n",
    "    name = type(model).__name__\n",
    "    model.fit(\n",
    "        dataset.flatten_data(dataset.X_train), \n",
    "        dataset.flatten_data(dataset.y_train)\n",
    "    )\n",
    "\n",
    "    y_test_pred = dataset.unflatten_data(\n",
    "        model.predict(\n",
    "            dataset.flatten_data(dataset.X_test)\n",
    "        )\n",
    "    )\n",
    "    mse = np.mean((y_test_pred - dataset.y_test)**2)\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(np.hstack(y_test_pred[::3][-10:]))\n",
    "    plt.title(name + \" \" + f\"MSE: {mse}\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(np.hstack(dataset.y_test[::3][-10:]))\n",
    "plt.title(\"Ground truth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d632f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bae140a1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Appendix: The Navier-Stokes equations\n",
    "\n",
    "The Navier-Stokes equations comprise a set of partial differential equations describing the evolution of a velocity field $\\mathbf{u}$,\n",
    "$$\n",
    "\\rho\\left(\\dfrac{\\partial \\mathbf{u}}{\\partial t} + \\mathbf{u} \\cdot \\nabla \\mathbf{u}       \\right) = -\\nabla p + \\mu \\nabla \\cdot \\left(    (\\nabla \\mathbf{u} + (\\nabla \\mathbf{u})^T) - \\dfrac{2}{3} (\\nabla \\cdot \\mathbf{u})\\mathbf{I}          \\right) + \\mathbf{F}\n",
    "$$\n",
    "This equation describes several interacting phenomena. The first parenthetical term proportional to the local density $\\rho$ corresponds to inertial forces, while the second term corresponds to pressure gradients in the fluid. The term proportional to the viscousity, $\\rho$, represents the effect of viscous drag within the fluid, while the forcing term $\\mathbf F$ corresponds to external forces.\n",
    "\n",
    "We usually supplement this partial differential equation with an additional equation describing conservation of momentum,\n",
    "$$\n",
    "\\dfrac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho \\mathbf{v}) = 0.\n",
    "$$\n",
    "Note that the full Navier-Stokes equations describe the dynamical evolution of three coupled fields: the velocity field $\\mathbf{u}$, the local density $\\rho$, and the pressure $p$. Usually, we will not attempt to directly solve the full equations, but instead we will first reduce these equations to a more specific form based on constraints or symmetries relevant to a specific problem. For example, when working with water or other liquids we usually assume the incompressibility condition $\\nabla \\rho = \\mathbf{0}$, $\\nabla \\cdot \\mathbf{u} = 0$.\n",
    "\n",
    "We will assume that we are working with an incompressible fluid with no body forces ($\\mathbf{F}=0$). If our flow has a characteristic length scale $L$ and velocity scale $U$, we can non-dimensionalize the Navier-Stokes equations by performing the substitution $\\mathbf{u} \\leftarrow \\mathbf{u} / U$, $\\mathbf{r} \\leftarrow \\mathbf{r} / L$, $\\nabla \\leftarrow L \\nabla$, $t \\leftarrow t (U / L)$, $p \\leftarrow p / (\\rho U^2)$, producing the non-dimensionalized Navier-Stokes equations\n",
    "$$\n",
    "{\\frac {\\partial \\mathbf {u} }{\\partial t}}+(\\mathbf {u} \\cdot \\nabla)\\mathbf {u} \\ =-\\nabla p+{\\frac {1}{Re}}\\nabla^{2}\\mathbf {u}.\n",
    "$$\n",
    "where we have used the identities $\\nabla \\cdot (\\nabla \\cdot \\mathbf {u} )\\mathbf {I} =\\nabla (\\nabla \\cdot \\mathbf {u} )$ and $\\nabla \\cdot (\\nabla \\mathbf{u})^T = \\nabla(\\nabla \\cdot \\mathbf{u})$\n",
    "\n",
    "The non-dimensional Reynolds number is defined as $Re \\equiv \\rho U L / \\mu$. The Reynolds number quantifies the ratio of inertial to viscous forces in the system. At large $Re$, the advective terms dominate the Navier-Stokes equations, while the Laplacian term associated with dissipation vanishes. The resulting inertia-dominated equations, known as the Euler equation for ideal (frictionless) fluids, give rise to turbulence in fast-moving classical fluids at large length scales (the Euler equation also approximates the dynamics of superfluids, for similar reasons). At very small $Re$, however, friction dominates the system, leading to the Stokes equations. This overdamped dynamical regime describes [many biological systems](http://www.damtp.cam.ac.uk/user/gold/pdfs/purcell.pdf), and it [many counterintuitive phenomena](https://www.youtube.com/watch?v=UpJ-kGII074) arise due to the appearance of time-reversal symmetry in the underlying equations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e888ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169637b0-6a6e-4e14-852a-d6115f698300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a073d124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473dcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9c8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ded97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e972983abb2b5c6293c34082f6ff1f6e60e8afbd2a068e0026ccecbb212fdb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
